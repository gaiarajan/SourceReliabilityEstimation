## Works Cited

Baly, Ramy, Karadzhov, Georgi, Alexandrov, Dimitar, James, Nakov, and Preslav. “Predicting Factuality of Reporting and Bias of News Media Sources.” arXiv.org, October 2, 2018. https://arxiv.org/abs/1810.01765.

In my project, I often used 'amount of work required on the part of the author' and 'amount of third-party engagement' as rough standards for level of veracity. To be clear, these aren't equivalent; however, papers that require a lot of work from the authors and many layers of vetting are generally more credible. This is why we often value 'peer-reviewed journals'. Therefore, since this paper had to go through multiple layers of third-party verification and reviewing in this high-level journal, it is more likely to be credible.
      
 Edell, Aaron. “I Trained Fake News Detection AI with >95% Accuracy, and Almost Went Crazy.” Medium. Towards Data Science, January 17, 2018. https://towardsdatascience.com/i-trained-fake-news-detection-ai-with-95-accuracy-and-almost-went-crazy-d10589aa57c.
 
This article was published by Towards Data Science, which has a [Twitter page](https://twitter.com/tdatascience?lang=en) that was first active in October 2016. Since this page is not verified, its existence would be a slight positive factor in source analysis. However, the page has 23.5K followers, and this engagement would be a major positive factor. 
  
 Horne, and Sibel. “This Just In: Fake News Packs a Lot in Title, Uses Simpler, Repetitive Content in Text Body, More Similar to Satire than Real News.” arXiv.org, March 28, 2017. https://arxiv.org/abs/1703.09398.
 
Again, this was published at the 2nd International Workshop on News and Public Opinion at ICWSM, which is a prestigious conference that necessitates third-party reviewers and several layers of verification.

Miguel M. Alvarez. “How Can Machine Learning and AI Help Solving the Fake News Problem?” The Practical Academic, November 21, 2017. https://miguelmalvarez.com/2017/03/23/how-can-machine-learning-and-ai-help-solving-the-fake-news-problem/.

My algorithm would find [this Twitter page](https://twitter.com/miguelmalvarez?lang=en) that links to the original website. Since this page has been active since December 2009 and has more than 1,000 followers, the algorithm would boost the factuality score. However, this blog does not have a Wikipedia page and it's not within the top 10,000 in the Alexa top sites ranking. Therefore, the score for this site would be 'moderate'. Because of [its author's qualifications](https://miguelmalvarez.com/about/), I as a human believe this source is factual.
  
  Vosoughi, Soroush, Deb Roy, and Sinan Aral. “The Spread of True and False News Online.” Science. American Association for the Advancement of Science, March 9, 2018. https://science.sciencemag.org/content/359/6380/1146.
  
Since this article was published in _Science_ by experienced researchers, both my algorithm and I would believe this source is factual.

Zhou, Xinyi, Zafarani, and Reza. “Fake News: A Survey of Research, Detection Methods, and Opportunities.” arXiv.org, December 2, 2018. https://arxiv.org/abs/1812.00315.

This article may not be as factual as the other ones because it was not published in a peer-reviewed journal: it was only published on arXiv, which has far fewer layers of review. However, the classifier is not able to distinguish the difference in its current form; this may be an avenue for future research.

(288 words source analysis)
